from collections.abc import AsyncGenerator
from typing import Any
from utils.response_util import ResponseFactory
from schemas.response import ContentKind
from infrastructure.logging.logger import logger

# åªæœ‰è¿™äº›èŠ‚ç‚¹çš„ LLM è¾“å‡ºæ‰é€ä¼ ç»™å‰ç«¯ä½œä¸ºæœ€ç»ˆç­”æ¡ˆ
_ANSWER_NODES = {"generate_report", "general_chat", "ask_user"}

async def process_workflow_stream(workflow_stream: AsyncGenerator) -> AsyncGenerator:
    """
    å¤„ç† LangGraph å¼‚æ­¥äº‹ä»¶æµå¹¶è½¬æ¢ä¸ºå‰ç«¯ SSE æ ¼å¼

    Args:
        workflow_stream: LangGraph astream_events ç”Ÿæˆå™¨
    """
    # è¿½è¸ªå½“å‰æ­£åœ¨æ‰§è¡Œçš„èŠ‚ç‚¹
    current_node: str = ""
    # å†…éƒ¨èŠ‚ç‚¹åç§°ï¼Œä¸å±•ç¤ºç»™å‰ç«¯
    _INTERNAL_NAMES = {"__start__", "__end__", "LangGraph"}
    # æ¡ä»¶è·¯ç”±å‡½æ•°åç§°ï¼Œä¸ä½œä¸ºèŠ‚ç‚¹å±•ç¤º
    _ROUTER_NAMES = {"route_intent", "route_slot_check", "route_verify_result",
                     "route_dispatch", "route_kb_check", "route_evaluate"}

    async for event in workflow_stream:
        kind = event.get("event")
        name = event.get("name")
        data = event.get("data")
        metadata = event.get("metadata", {})

        # â”€â”€ 1. èŠ‚ç‚¹å¼€å§‹ â†’ PROCESS æç¤º â”€â”€
        # astream_events v2 ä¸­èŠ‚ç‚¹äº‹ä»¶ä¸º on_chain_start/on_chain_end
        if kind == "on_chain_start":
            lg_node = metadata.get("langgraph_node", "")
            # åªå¤„ç† LangGraph èŠ‚ç‚¹ï¼ˆæœ‰ langgraph_node å…ƒæ•°æ®ï¼‰ï¼Œè·³è¿‡å†…éƒ¨å’Œè·¯ç”±
            if lg_node and name == lg_node and name not in _INTERNAL_NAMES and name not in _ROUTER_NAMES:
                current_node = name
                yield "data: " + ResponseFactory.build_text(
                    f"ğŸ”„ è¿›å…¥ç¯èŠ‚: {name}", ContentKind.PROCESS
                ).model_dump_json() + "\n\n"

        # â”€â”€ 2. èŠ‚ç‚¹ç»“æŸ â†’ æå–ç­”æ¡ˆèŠ‚ç‚¹çš„å›å¤ â”€â”€
        elif kind == "on_chain_end":
            lg_node = metadata.get("langgraph_node", "")

            # è¿½é—®èŠ‚ç‚¹ï¼šä» output state å–å‡ºè¿½é—®å†…å®¹
            if name == "ask_user" and lg_node == "ask_user":
                output = data.get("output", {})
                messages = output.get("messages", [])
                pending_intent = output.get("current_intent", "")
                for msg in messages:
                    content = msg.content if hasattr(msg, "content") else str(msg)
                    if content:
                        import json as _json
                        packet = ResponseFactory.build_text(content, ContentKind.ANSWER).model_dump()
                        packet["is_ask_user"] = True
                        packet["pending_intent"] = pending_intent
                        yield "data: " + _json.dumps(packet, ensure_ascii=False) + "\n\n"

            # å…¶ä»–ç­”æ¡ˆèŠ‚ç‚¹ï¼ˆgeneral_chat, generate_reportï¼‰ï¼šä» output messages æå–å›å¤
            elif name in _ANSWER_NODES and lg_node == name and name != "ask_user":
                output = data.get("output", {})
                messages = output.get("messages", [])
                for msg in messages:
                    content = msg.content if hasattr(msg, "content") else str(msg)
                    if content:
                        yield "data: " + ResponseFactory.build_text(
                            content, ContentKind.ANSWER
                        ).model_dump_json() + "\n\n"

            if lg_node and name == lg_node:
                current_node = ""

        # â”€â”€ 3. LLM æµå¼è¾“å‡ºï¼ˆå¦‚æœèŠ‚ç‚¹ä½¿ç”¨ astream è°ƒç”¨ LLMï¼‰â”€â”€
        elif kind == "on_chat_model_stream":
            chunk = data.get("chunk", None)
            if not chunk:
                continue

            source_node = metadata.get("langgraph_node", current_node)

            # æå–æ¨ç†å†…å®¹ (Thinking)
            reasoning = None
            if hasattr(chunk, "reasoning_content") and chunk.reasoning_content:
                reasoning = chunk.reasoning_content
            elif hasattr(chunk, "additional_kwargs") and "reasoning_content" in chunk.additional_kwargs:
                reasoning = chunk.additional_kwargs["reasoning_content"]

            if reasoning:
                yield "data: " + ResponseFactory.build_text(
                    reasoning, ContentKind.THINKING
                ).model_dump_json() + "\n\n"

            # æµå¼ç­”æ¡ˆï¼ˆä»…ç­”æ¡ˆèŠ‚ç‚¹ï¼‰
            content = chunk.content if hasattr(chunk, "content") else None
            if content and source_node in _ANSWER_NODES:
                yield "data: " + ResponseFactory.build_text(
                    content, ContentKind.ANSWER
                ).model_dump_json() + "\n\n"

        # â”€â”€ 4. å·¥å…·è°ƒç”¨ â”€â”€
        elif kind == "on_tool_start":
            yield "data: " + ResponseFactory.build_text(
                f"ğŸ”§ è°ƒç”¨å·¥å…·: {name}", ContentKind.PROCESS
            ).model_dump_json() + "\n\n"

        # â”€â”€ 5. è‡ªå®šä¹‰äº‹ä»¶ï¼ˆé¢„ç•™æ‰©å±•ï¼‰â”€â”€
        elif kind == "on_custom_event":
            pass

    # 5. å‘é€ç»“æŸä¿¡å·
    yield "data: " + ResponseFactory.build_finish().model_dump_json() + "\n\n"