"""
ES æ··åˆæ£€ç´¢æœåŠ¡ (RAG V2.0)
å®ç°åŸºäº Elasticsearch çš„æ··åˆæ£€ç´¢ï¼šBM25 å…³é”®è¯æ£€ç´¢ + å‘é‡è¯­ä¹‰æ£€ç´¢
"""

from typing import List, Dict, Any, Optional

try:
    from infrastructure.es_client import ESClient
    from services.embedding_service import EmbeddingService
    from services.text_processor import TextProcessor
    from config.settings import settings
    from infrastructure.logger import logger
except ModuleNotFoundError:
    import sys
    import os

    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    sys.path.insert(0, project_root)
    from infrastructure.es_client import ESClient
    from services.embedding_service import EmbeddingService
    from services.text_processor import TextProcessor
    from config.settings import settings
    from infrastructure.logger import logger


class ESRetrievalService:
    """
    ES æ··åˆæ£€ç´¢æœåŠ¡ (RAG V2.0)

    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. æ··åˆæ£€ç´¢ï¼šBM25 å…³é”®è¯åŒ¹é… + KNN å‘é‡è¯­ä¹‰æ£€ç´¢
    2. Collapse æŠ˜å ï¼šæŒ‰ knowledge_no å»é‡ï¼Œè¿”å›ä¸é‡å¤çš„çŸ¥è¯†ç‚¹
    3. çˆ¶å­æ–‡æ¡£ï¼šæ£€ç´¢ç”¨ Chunkï¼Œå±•ç¤ºç”¨ Parent
    """

    def __init__(self):
        """åˆå§‹åŒ–æ£€ç´¢æœåŠ¡"""
        self.es_client = ESClient()
        self.embedding_service = EmbeddingService()
        self.text_processor = TextProcessor()
        self.index_name = settings.ES_INDEX_NAME
        logger.info(f"âœ… ES æ··åˆæ£€ç´¢æœåŠ¡åˆå§‹åŒ–æˆåŠŸï¼Œç´¢å¼•: {self.index_name}")

    def hybrid_search(
        self,
        query: str,
        top_k: int = 5,
        keyword_weight: float = 0.5,
        vector_weight: float = 0.5,
    ) -> List[Dict[str, Any]]:
        """
        æ··åˆæ£€ç´¢ï¼šBM25 + å‘é‡æ£€ç´¢ + Collapse æŠ˜å 

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›çš„çŸ¥è¯†ç‚¹æ•°é‡
            keyword_weight: å…³é”®è¯æ£€ç´¢æƒé‡
            vector_weight: å‘é‡æ£€ç´¢æƒé‡

        Returns:
            List[Dict]: æ£€ç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« knowledge_no å’Œ score
        """
        try:
            # 1. é¢„å¤„ç†æŸ¥è¯¢
            query_segmented = self.text_processor.segment_chinese(query)
            query_vector = self.embedding_service.embed_text(query)

            logger.info(f"ğŸ” å¼€å§‹æ··åˆæ£€ç´¢: {query}")
            logger.info(f"   åˆ†è¯ç»“æœ: {query_segmented[:100]}...")

            # 2. æ„é€ æ··åˆæ£€ç´¢ DSL
            search_body = self._build_hybrid_search_dsl(
                query_segmented=query_segmented,
                query_vector=query_vector,
                top_k=top_k,
                keyword_weight=keyword_weight,
                vector_weight=vector_weight,
            )

            # 3. æ‰§è¡Œæ£€ç´¢
            response = self.es_client.search(self.index_name, search_body)

            # 4. è§£æç»“æœ
            results = self._parse_search_results(response)

            logger.info(f"âœ… æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(results)} ä¸ªçŸ¥è¯†ç‚¹")
            return results

        except Exception as e:
            logger.error(f"âŒ æ··åˆæ£€ç´¢å¤±è´¥: {e}")
            raise

    def _build_hybrid_search_dsl(
        self,
        query_segmented: str,
        query_vector: List[float],
        top_k: int,
        keyword_weight: float,
        vector_weight: float,
    ) -> Dict[str, Any]:
        """
        æ„é€ æ··åˆæ£€ç´¢ DSL æŸ¥è¯¢

        æ ¸å¿ƒé€»è¾‘ï¼š
        1. Filter: åªæ£€ç´¢ doc_type=chunk
        2. Should: BM25 å…³é”®è¯ + KNN å‘é‡
        3. Collapse: æŒ‰ knowledge_no æŠ˜å å»é‡

        Args:
            query_segmented: åˆ†è¯åçš„æŸ¥è¯¢
            query_vector: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›æ•°é‡
            keyword_weight: å…³é”®è¯æƒé‡
            vector_weight: å‘é‡æƒé‡

        Returns:
            Dict: ES DSL æŸ¥è¯¢ä½“
        """
        dsl = {
            "size": top_k * 3,  # æŠ˜å å‰å¤šå¬å›ä¸€äº›
            "query": {
                "bool": {
                    "filter": [{"term": {"doc_type": "chunk"}}],  # åªæ£€ç´¢ chunk
                    "should": [
                        # è·¯å¾„ 1: BM25 å…³é”®è¯æ£€ç´¢
                        {
                            "multi_match": {
                                "query": query_segmented,
                                "fields": ["title^2", "content"],  # title æƒé‡ 2 å€
                                "type": "best_fields",
                                "boost": keyword_weight,
                            }
                        },
                        # è·¯å¾„ 2: KNN å‘é‡æ£€ç´¢
                        {
                            "script_score": {
                                "query": {"match_all": {}},
                                "script": {
                                    "source": "cosineSimilarity(params.query_vector, 'content_vector') + 1.0",
                                    "params": {"query_vector": query_vector},
                                },
                                "boost": vector_weight,
                            }
                        },
                    ],
                    "minimum_should_match": 1,
                }
            },
            # Collapse æŠ˜å ï¼šæŒ‰ knowledge_no å»é‡
            "collapse": {"field": "knowledge_no"},
            # è¿”å›å­—æ®µ
            "_source": ["doc_id", "knowledge_no", "title", "content", "chunk_index"],
        }

        return dsl

    def _parse_search_results(self, response: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        è§£æ ES æ£€ç´¢ç»“æœ

        Args:
            response: ES å“åº”

        Returns:
            List[Dict]: è§£æåçš„ç»“æœåˆ—è¡¨
        """
        results = []
        hits = response.get("hits", {}).get("hits", [])

        for hit in hits:
            source = hit.get("_source", {})
            results.append(
                {
                    "knowledge_no": source.get("knowledge_no"),
                    "doc_id": source.get("doc_id"),
                    "title": source.get("title"),
                    "content": source.get("content"),
                    "chunk_index": source.get("chunk_index"),
                    "score": hit.get("_score", 0),
                }
            )

        return results

    def get_parent_documents(self, knowledge_nos: List[str]) -> Dict[str, str]:
        """
        æ‰¹é‡è·å–çˆ¶æ–‡æ¡£çš„å®Œæ•´å†…å®¹

        Args:
            knowledge_nos: çŸ¥è¯†ç‚¹ ID åˆ—è¡¨

        Returns:
            Dict[str, str]: {knowledge_no: full_content}
        """
        try:
            if not knowledge_nos:
                return {}

            # æ„é€ æ‰¹é‡æŸ¥è¯¢
            parent_ids = [f"{kno}_parent" for kno in knowledge_nos]

            # ä½¿ç”¨ mget æ‰¹é‡è·å–
            docs = self.es_client.mget(self.index_name, parent_ids)

            # è§£æç»“æœ
            # æ³¨æ„ï¼šes_client.mget() è¿”å›çš„æ˜¯ _source å†…å®¹åˆ—è¡¨ï¼Œä¸æ˜¯å®Œæ•´æ–‡æ¡£ç»“æ„
            result = {}
            for doc in docs:
                if doc:  # doc å·²ç»æ˜¯ _source çš„å†…å®¹
                    knowledge_no = doc.get("knowledge_no")
                    full_content = doc.get("full_content", "")
                    if knowledge_no and full_content:
                        result[knowledge_no] = full_content

            logger.info(f"âœ… è·å–äº† {len(result)} ä¸ªçˆ¶æ–‡æ¡£")
            return result

        except Exception as e:
            logger.error(f"âŒ è·å–çˆ¶æ–‡æ¡£å¤±è´¥: {e}")
            return {}

    def _keyword_search(self, query_segmented: str, top_k: int) -> List[Dict[str, Any]]:
        """
        çº¯å…³é”®è¯æ£€ç´¢ï¼ˆBM25ï¼‰

        Args:
            query_segmented: åˆ†è¯åçš„æŸ¥è¯¢
            top_k: è¿”å›æ•°é‡

        Returns:
            List[Dict]: æ£€ç´¢ç»“æœ
        """
        dsl = {
            "size": top_k,
            "query": {
                "bool": {
                    "filter": [{"term": {"doc_type": "chunk"}}],
                    "must": [
                        {
                            "multi_match": {
                                "query": query_segmented,
                                "fields": ["title^2", "content"],
                                "type": "best_fields",
                            }
                        }
                    ],
                }
            },
            "_source": ["doc_id", "knowledge_no", "title", "content", "chunk_index"],
        }

        response = self.es_client.search(self.index_name, dsl)
        return self._parse_search_results(response)

    def _vector_search(
        self, query_vector: List[float], top_k: int
    ) -> List[Dict[str, Any]]:
        """
        çº¯å‘é‡æ£€ç´¢ï¼ˆKNNï¼‰

        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›æ•°é‡

        Returns:
            List[Dict]: æ£€ç´¢ç»“æœ
        """
        dsl = {
            "size": top_k,
            "query": {
                "bool": {
                    "filter": [{"term": {"doc_type": "chunk"}}],
                    "must": [
                        {
                            "script_score": {
                                "query": {"match_all": {}},
                                "script": {
                                    "source": "cosineSimilarity(params.query_vector, 'content_vector') + 1.0",
                                    "params": {"query_vector": query_vector},
                                },
                            }
                        }
                    ],
                }
            },
            "_source": ["doc_id", "knowledge_no", "title", "content", "chunk_index"],
        }

        response = self.es_client.search(self.index_name, dsl)
        return self._parse_search_results(response)

    def _rrf_fusion(
        self,
        ranking_lists: List[List[Dict[str, Any]]],
        k: int = 60,
    ) -> List[Dict[str, Any]]:
        """
        é€šç”¨çš„ RRF (Reciprocal Rank Fusion) èåˆæ’åºç®—æ³•

        æ ¸å¿ƒå…¬å¼ï¼šscore(doc) = Î£ 1/(k + rank_i(doc))
        å…¶ä¸­ k=60 æ˜¯å¸¸ç”¨çš„å¹³æ»‘å‚æ•°

        Args:
            ranking_lists: å¤šä¸ªæ’åºåˆ—è¡¨çš„åˆ—è¡¨ã€‚æ¯ä¸ªå†…éƒ¨åˆ—è¡¨ä»£è¡¨ä¸€è·¯æ£€ç´¢ç»“æœ (Rank 1..N)
            k: RRF å¹³æ»‘å‚æ•°ï¼ˆé»˜è®¤ 60ï¼‰

        Returns:
            List[Dict]: èåˆåçš„ç»“æœåˆ—è¡¨
        """
        rrf_scores = {}

        # éå†æ¯ä¸€è·¯æ£€ç´¢ç»“æœï¼ˆæŠ•ç¥¨è€…ï¼‰
        for ranking_list in ranking_lists:
            # éå†è¯¥è·¯ç»“æœä¸­çš„æ¯ä¸ªæ–‡æ¡£ï¼ŒRank ä» 1 å¼€å§‹
            for rank, result in enumerate(ranking_list, start=1):
                knowledge_no = result["knowledge_no"]
                score = 1.0 / (k + rank)
                
                if knowledge_no not in rrf_scores:
                    # åˆå§‹åŒ–æ–‡æ¡£ä¿¡æ¯
                    rrf_scores[knowledge_no] = {
                        "knowledge_no": knowledge_no,
                        "doc_id": result["doc_id"],
                        "title": result["title"],
                        "content": result["content"],
                        "chunk_index": result["chunk_index"],
                        "rrf_score": 0.0,
                        # è®°å½•æ¯ä¸€è·¯çš„å‘½ä¸­æƒ…å†µ (å¯é€‰ï¼Œç”¨äºè°ƒè¯•)
                        "hit_count": 0
                    }
                
                # ç´¯åŠ åˆ†æ•°
                rrf_scores[knowledge_no]["rrf_score"] += score
                rrf_scores[knowledge_no]["hit_count"] += 1

        # æŒ‰ RRF åˆ†æ•°å€’åºæ’åˆ—
        sorted_results = sorted(
            rrf_scores.values(), key=lambda x: x["rrf_score"], reverse=True
        )

        logger.info(f"âœ… RRF èåˆå®Œæˆ: èšåˆäº† {len(ranking_lists)} è·¯ç»“æœï¼Œç”Ÿæˆ {len(sorted_results)} ä¸ªå”¯ä¸€çŸ¥è¯†ç‚¹")
        return sorted_results

    def rrf_search(
        self, query: str, top_k: int = 5, rrf_k: int = 60
    ) -> List[Dict[str, Any]]:
        """
        åŸºäº RRF çš„æ··åˆæ£€ç´¢ï¼ˆå• Queryï¼‰

        æµç¨‹ï¼š
        1. åˆ†åˆ«æ‰§è¡Œ BM25 å…³é”®è¯æ£€ç´¢å’Œå‘é‡æ£€ç´¢
        2. ä½¿ç”¨ RRF ç®—æ³•èåˆä¸¤è·¯ç»“æœ

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›çš„çŸ¥è¯†ç‚¹æ•°é‡
            rrf_k: RRF å¹³æ»‘å‚æ•°

        Returns:
            List[Dict]: æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # 1. é¢„å¤„ç†æŸ¥è¯¢
            query_segmented = self.text_processor.segment_chinese(query)
            query_vector = self.embedding_service.embed_query(query)

            logger.info(f"ğŸ” å¼€å§‹ RRF æ··åˆæ£€ç´¢: {query}")

            # 2. åˆ†åˆ«æ‰§è¡Œä¸¤è·¯æ£€ç´¢ï¼ˆå¬å›æ›´å¤šå€™é€‰ï¼‰
            # æ³¨æ„ï¼šæ¯è·¯ç‹¬ç«‹å¬å› top_k * 3
            keyword_results = self._keyword_search(query_segmented, top_k=top_k * 3)
            vector_results = self._vector_search(query_vector, top_k=top_k * 3)

            logger.info(f"   å…³é”®è¯æ£€ç´¢: {len(keyword_results)} ä¸ªç»“æœ")
            logger.info(f"   å‘é‡æ£€ç´¢: {len(vector_results)} ä¸ªç»“æœ")

            # 3. RRF èåˆæ’åº
            # ä¼ å…¥ä¸¤ä¸ªåˆ—è¡¨ï¼š[BM25ç»“æœ, Vectorç»“æœ]
            fused_results = self._rrf_fusion([keyword_results, vector_results], k=rrf_k)

            # 4. å– Top-K
            final_results = fused_results[:top_k]

            return final_results

        except Exception as e:
            logger.error(f"âŒ RRF æ£€ç´¢å¤±è´¥: {e}")
            raise

    def multi_query_rrf_search(
        self, queries: List[str], top_k: int = 5, rrf_k: int = 60
    ) -> List[Dict[str, Any]]:
        """
        å¤šè·¯æŸ¥è¯¢ RRF èåˆæ£€ç´¢

        æµç¨‹ï¼š
        1. éå†æ¯ä¸ªæŸ¥è¯¢è¯­å¥ï¼ˆå¦‚ï¼šåŸå§‹ Query å’Œ é‡å†™ Queryï¼‰
        2. å¯¹æ¯ä¸ªæŸ¥è¯¢åˆ†åˆ«æ‰§è¡Œå…³é”®è¯æ£€ç´¢å’Œå‘é‡æ£€ç´¢
        3. å°†æ‰€æœ‰æ£€ç´¢è·¯ï¼ˆQueryæ•° * 2ï¼‰ä½œä¸ºç‹¬ç«‹çš„æŠ•ç¥¨åˆ—è¡¨ï¼ŒæŠ•å…¥ RRF å®¹å™¨è¿›è¡Œç»Ÿä¸€èåˆ

        Args:
            queries: æŸ¥è¯¢è¯­å¥åˆ—è¡¨
            top_k: æœ€ç»ˆè¿”å›çš„æ•°é‡
            rrf_k: RRF å¹³æ»‘å‚æ•°

        Returns:
            List[Dict]: èåˆåçš„æœ€ä¼˜ç»“æœ
        """
        try:
            all_ranking_lists = []

            for q in queries:
                if not q.strip():
                    continue
                
                logger.info(f"ğŸ” [å¤šè·¯æ£€ç´¢] å¤„ç†å­æŸ¥è¯¢: {q}")
                
                # é¢„å¤„ç†
                query_segmented = self.text_processor.segment_chinese(q)
                query_vector = self.embedding_service.embed_query(q)
                
                # åˆ†åˆ«å¬å›
                kw_res = self._keyword_search(query_segmented, top_k=top_k * 3)
                vec_res = self._vector_search(query_vector, top_k=top_k * 3)
                
                # å°†æ¯ä¸€è·¯ç»“æœä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„åˆ—è¡¨åŠ å…¥
                if kw_res:
                    all_ranking_lists.append(kw_res)
                if vec_res:
                    all_ranking_lists.append(vec_res)

            # æ‰§è¡Œ RRF èåˆ
            # æ­¤æ—¶ all_ranking_lists åŒ…å« (Queryæ•° * 2) ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªåˆ—è¡¨çš„ Rank éƒ½æ˜¯ä» 1 å¼€å§‹
            fused_results = self._rrf_fusion(all_ranking_lists, k=rrf_k)

            # å– Top-K
            final_results = fused_results[:top_k]
            
            logger.info(f"âœ… å¤šè·¯ RRF æ£€ç´¢å®Œæˆï¼Œèåˆäº† {len(queries)} ä¸ªæŸ¥è¯¢ ({len(all_ranking_lists)} è·¯ç»“æœ)ï¼Œè¿”å› {len(final_results)} ä¸ªçŸ¥è¯†ç‚¹")
            return final_results

        except Exception as e:
            logger.error(f"âŒ å¤šè·¯ RRF æ£€ç´¢å¤±è´¥: {e}")
            raise

    def retrieve(
        self, query: Any, top_k: int = 5, return_full_content: bool = True
    ) -> List[Dict[str, Any]]:
        """
        å®Œæ•´çš„æ£€ç´¢æµç¨‹ (æ”¯æŒå• Query æˆ– Multi-Query)

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢ (str æˆ– List[str])
            top_k: è¿”å›çš„çŸ¥è¯†ç‚¹æ•°é‡
            return_full_content: æ˜¯å¦è¿”å›å®Œæ•´å†…å®¹ï¼ˆParentï¼‰

        Returns:
            List[Dict]: æ£€ç´¢ç»“æœ
        """
        try:
            # 1. ç¡®å®šæ£€ç´¢æ–¹æ¡ˆ
            if isinstance(query, list):
                search_results = self.multi_query_rrf_search(query, top_k=top_k)
            else:
                search_results = self.rrf_search(query, top_k=top_k)

            if not search_results:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡æ¡£")
                return []

            # 2. æå– knowledge_no
            knowledge_nos = [r["knowledge_no"] for r in search_results]

            # 3. è·å–çˆ¶æ–‡æ¡£
            if return_full_content:
                parent_docs = self.get_parent_documents(knowledge_nos)

                # 4. åˆå¹¶ç»“æœ
                for result in search_results:
                    kno = result["knowledge_no"]
                    result["full_content"] = parent_docs.get(kno, "")

            return search_results

        except Exception as e:
            logger.error(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
            raise


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import sys
    import os
    from infrastructure.logger import logger

    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    sys.path.insert(0, project_root)

    print("\n" + "=" * 60)
    print("æµ‹è¯• ES æ··åˆæ£€ç´¢æœåŠ¡ (RAG V2.0)")
    print("=" * 60)

    # åˆå§‹åŒ–æœåŠ¡
    service = ESRetrievalService()

    # æµ‹è¯•æŸ¥è¯¢
    test_queries = ["è”æƒ³æ‰‹æœºK900å¦‚ä½•æ’æ‹”SIMå¡", "ç”µæ± ç»­èˆªé—®é¢˜", "å¦‚ä½•è¿æ¥WiFi"]

    for query in test_queries:
        print(f"\n{'='*60}")
        print(f"æŸ¥è¯¢: {query}")
        print(f"{'='*60}")

        try:
            results = service.retrieve(query, top_k=3)

            if results:
                print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªç»“æœ:\n")
                for i, result in enumerate(results, 1):
                    print(f"--- ç»“æœ {i} ---")
                    print(f"Knowledge No: {result['knowledge_no']}")
                    print(f"æ ‡é¢˜: {result['title'][:50]}...")
                    print(f"åˆ†æ•°: {result['score']:.4f}")
                    print(f"å®Œæ•´å†…å®¹é•¿åº¦: {len(result.get('full_content', ''))} å­—ç¬¦")
                    print()
            else:
                print("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…ç»“æœ")

        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
