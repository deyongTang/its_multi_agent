"""
ES æ··åˆæ£€ç´¢æœåŠ¡ (RAG V2.0)
å®ç°åŸºäº Elasticsearch çš„æ··åˆæ£€ç´¢ï¼šBM25 å…³é”®è¯æ£€ç´¢ + å‘é‡è¯­ä¹‰æ£€ç´¢
"""

from typing import List, Dict, Any, Optional

try:
    from infrastructure.es_client import ESClient
    from services.embedding_service import EmbeddingService
    from services.text_processor import TextProcessor
    from config.settings import settings
    from infrastructure.logger import logger
except ModuleNotFoundError:
    import sys
    import os

    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    sys.path.insert(0, project_root)
    from infrastructure.es_client import ESClient
    from services.embedding_service import EmbeddingService
    from services.text_processor import TextProcessor
    from config.settings import settings
    from infrastructure.logger import logger


class ESRetrievalService:
    """
    ES æ··åˆæ£€ç´¢æœåŠ¡ (RAG V2.0)

    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. æ··åˆæ£€ç´¢ï¼šBM25 å…³é”®è¯åŒ¹é… + KNN å‘é‡è¯­ä¹‰æ£€ç´¢
    2. Collapse æŠ˜å ï¼šæŒ‰ knowledge_no å»é‡ï¼Œè¿”å›ä¸é‡å¤çš„çŸ¥è¯†ç‚¹
    3. çˆ¶å­æ–‡æ¡£ï¼šæ£€ç´¢ç”¨ Chunkï¼Œå±•ç¤ºç”¨ Parent
    """

    def __init__(self):
        """åˆå§‹åŒ–æ£€ç´¢æœåŠ¡"""
        self.es_client = ESClient()
        self.embedding_service = EmbeddingService()
        self.text_processor = TextProcessor()
        self.index_name = settings.ES_INDEX_NAME
        logger.info(f"âœ… ES æ··åˆæ£€ç´¢æœåŠ¡åˆå§‹åŒ–æˆåŠŸï¼Œç´¢å¼•: {self.index_name}")

    def hybrid_search(
        self,
        query: str,
        top_k: int = 5,
        keyword_weight: float = 0.5,
        vector_weight: float = 0.5,
    ) -> List[Dict[str, Any]]:
        """
        æ··åˆæ£€ç´¢ï¼šBM25 + å‘é‡æ£€ç´¢ + Collapse æŠ˜å 

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›çš„çŸ¥è¯†ç‚¹æ•°é‡
            keyword_weight: å…³é”®è¯æ£€ç´¢æƒé‡
            vector_weight: å‘é‡æ£€ç´¢æƒé‡

        Returns:
            List[Dict]: æ£€ç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« knowledge_no å’Œ score
        """
        try:
            # 1. é¢„å¤„ç†æŸ¥è¯¢
            query_segmented = self.text_processor.segment_chinese(query)
            query_vector = self.embedding_service.embed_text(query)

            logger.info(f"ğŸ” å¼€å§‹æ··åˆæ£€ç´¢: {query}")
            logger.info(f"   åˆ†è¯ç»“æœ: {query_segmented[:100]}...")

            # 2. æ„é€ æ··åˆæ£€ç´¢ DSL
            search_body = self._build_hybrid_search_dsl(
                query_segmented=query_segmented,
                query_vector=query_vector,
                top_k=top_k,
                keyword_weight=keyword_weight,
                vector_weight=vector_weight,
            )

            # 3. æ‰§è¡Œæ£€ç´¢
            response = self.es_client.search(self.index_name, search_body)

            # 4. è§£æç»“æœ
            results = self._parse_search_results(response)

            logger.info(f"âœ… æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(results)} ä¸ªçŸ¥è¯†ç‚¹")
            return results

        except Exception as e:
            logger.error(f"âŒ æ··åˆæ£€ç´¢å¤±è´¥: {e}")
            raise

    def _build_hybrid_search_dsl(
        self,
        query_segmented: str,
        query_vector: List[float],
        top_k: int,
        keyword_weight: float,
        vector_weight: float,
    ) -> Dict[str, Any]:
        """
        æ„é€ æ··åˆæ£€ç´¢ DSL æŸ¥è¯¢

        æ ¸å¿ƒé€»è¾‘ï¼š
        1. Filter: åªæ£€ç´¢ doc_type=chunk
        2. Should: BM25 å…³é”®è¯ + KNN å‘é‡
        3. Collapse: æŒ‰ knowledge_no æŠ˜å å»é‡

        Args:
            query_segmented: åˆ†è¯åçš„æŸ¥è¯¢
            query_vector: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›æ•°é‡
            keyword_weight: å…³é”®è¯æƒé‡
            vector_weight: å‘é‡æƒé‡

        Returns:
            Dict: ES DSL æŸ¥è¯¢ä½“
        """
        dsl = {
            "size": top_k * 3,  # æŠ˜å å‰å¤šå¬å›ä¸€äº›
            "query": {
                "bool": {
                    "filter": [{"term": {"doc_type": "chunk"}}],  # åªæ£€ç´¢ chunk
                    "should": [
                        # è·¯å¾„ 1: BM25 å…³é”®è¯æ£€ç´¢
                        {
                            "multi_match": {
                                "query": query_segmented,
                                "fields": ["title^2", "content"],  # title æƒé‡ 2 å€
                                "type": "best_fields",
                                "boost": keyword_weight,
                            }
                        },
                        # è·¯å¾„ 2: KNN å‘é‡æ£€ç´¢
                        {
                            "script_score": {
                                "query": {"match_all": {}},
                                "script": {
                                    "source": "cosineSimilarity(params.query_vector, 'content_vector') + 1.0",
                                    "params": {"query_vector": query_vector},
                                },
                                "boost": vector_weight,
                            }
                        },
                    ],
                    "minimum_should_match": 1,
                }
            },
            # Collapse æŠ˜å ï¼šæŒ‰ knowledge_no å»é‡
            "collapse": {"field": "knowledge_no"},
            # è¿”å›å­—æ®µ
            "_source": ["doc_id", "knowledge_no", "title", "content", "chunk_index"],
        }

        return dsl

    def _parse_search_results(self, response: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        è§£æ ES æ£€ç´¢ç»“æœ

        Args:
            response: ES å“åº”

        Returns:
            List[Dict]: è§£æåçš„ç»“æœåˆ—è¡¨
        """
        results = []
        hits = response.get("hits", {}).get("hits", [])

        for hit in hits:
            source = hit.get("_source", {})
            results.append(
                {
                    "knowledge_no": source.get("knowledge_no"),
                    "doc_id": source.get("doc_id"),
                    "title": source.get("title"),
                    "content": source.get("content"),
                    "chunk_index": source.get("chunk_index"),
                    "score": hit.get("_score", 0),
                }
            )

        return results

    def get_parent_documents(self, knowledge_nos: List[str]) -> Dict[str, str]:
        """
        æ‰¹é‡è·å–çˆ¶æ–‡æ¡£çš„å®Œæ•´å†…å®¹

        Args:
            knowledge_nos: çŸ¥è¯†ç‚¹ ID åˆ—è¡¨

        Returns:
            Dict[str, str]: {knowledge_no: full_content}
        """
        try:
            if not knowledge_nos:
                return {}

            # æ„é€ æ‰¹é‡æŸ¥è¯¢
            parent_ids = [f"{kno}_parent" for kno in knowledge_nos]

            # ä½¿ç”¨ mget æ‰¹é‡è·å–
            docs = self.es_client.mget(self.index_name, parent_ids)

            # è§£æç»“æœ
            # æ³¨æ„ï¼šes_client.mget() è¿”å›çš„æ˜¯ _source å†…å®¹åˆ—è¡¨ï¼Œä¸æ˜¯å®Œæ•´æ–‡æ¡£ç»“æ„
            result = {}
            for doc in docs:
                if doc:  # doc å·²ç»æ˜¯ _source çš„å†…å®¹
                    knowledge_no = doc.get("knowledge_no")
                    full_content = doc.get("full_content", "")
                    if knowledge_no and full_content:
                        result[knowledge_no] = full_content

            logger.info(f"âœ… è·å–äº† {len(result)} ä¸ªçˆ¶æ–‡æ¡£")
            return result

        except Exception as e:
            logger.error(f"âŒ è·å–çˆ¶æ–‡æ¡£å¤±è´¥: {e}")
            return {}

    def _keyword_search(self, query_segmented: str, top_k: int) -> List[Dict[str, Any]]:
        """
        çº¯å…³é”®è¯æ£€ç´¢ï¼ˆBM25ï¼‰

        Args:
            query_segmented: åˆ†è¯åçš„æŸ¥è¯¢
            top_k: è¿”å›æ•°é‡

        Returns:
            List[Dict]: æ£€ç´¢ç»“æœ
        """
        dsl = {
            "size": top_k,
            "query": {
                "bool": {
                    "filter": [{"term": {"doc_type": "chunk"}}],
                    "must": [
                        {
                            "multi_match": {
                                "query": query_segmented,
                                "fields": ["title^2", "content"],
                                "type": "best_fields",
                            }
                        }
                    ],
                }
            },
            "_source": ["doc_id", "knowledge_no", "title", "content", "chunk_index"],
        }

        response = self.es_client.search(self.index_name, dsl)
        return self._parse_search_results(response)

    def _vector_search(
        self, query_vector: List[float], top_k: int
    ) -> List[Dict[str, Any]]:
        """
        çº¯å‘é‡æ£€ç´¢ï¼ˆKNNï¼‰

        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›æ•°é‡

        Returns:
            List[Dict]: æ£€ç´¢ç»“æœ
        """
        dsl = {
            "size": top_k,
            "query": {
                "bool": {
                    "filter": [{"term": {"doc_type": "chunk"}}],
                    "must": [
                        {
                            "script_score": {
                                "query": {"match_all": {}},
                                "script": {
                                    "source": "cosineSimilarity(params.query_vector, 'content_vector') + 1.0",
                                    "params": {"query_vector": query_vector},
                                },
                            }
                        }
                    ],
                }
            },
            "_source": ["doc_id", "knowledge_no", "title", "content", "chunk_index"],
        }

        response = self.es_client.search(self.index_name, dsl)
        return self._parse_search_results(response)

    def _rrf_fusion(
        self,
        keyword_results: List[Dict[str, Any]],
        vector_results: List[Dict[str, Any]],
        k: int = 60,
    ) -> List[Dict[str, Any]]:
        """
        RRF (Reciprocal Rank Fusion) èåˆæ’åºç®—æ³•

        æ ¸å¿ƒå…¬å¼ï¼šscore(doc) = Î£ 1/(k + rank_i(doc))
        å…¶ä¸­ k=60 æ˜¯å¸¸ç”¨çš„å¹³æ»‘å‚æ•°

        Args:
            keyword_results: BM25 å…³é”®è¯æ£€ç´¢ç»“æœ
            vector_results: å‘é‡æ£€ç´¢ç»“æœ
            k: RRF å¹³æ»‘å‚æ•°ï¼ˆé»˜è®¤ 60ï¼‰

        Returns:
            List[Dict]: èåˆåçš„ç»“æœåˆ—è¡¨
        """
        rrf_scores = {}

        # 1. è®¡ç®—å…³é”®è¯æ£€ç´¢çš„ RRF åˆ†æ•°
        for rank, result in enumerate(keyword_results, start=1):
            knowledge_no = result["knowledge_no"]
            score = 1.0 / (k + rank)
            if knowledge_no not in rrf_scores:
                rrf_scores[knowledge_no] = {
                    "knowledge_no": knowledge_no,
                    "doc_id": result["doc_id"],
                    "title": result["title"],
                    "content": result["content"],
                    "chunk_index": result["chunk_index"],
                    "rrf_score": 0.0,
                    "keyword_rank": rank,
                    "vector_rank": None,
                }
            rrf_scores[knowledge_no]["rrf_score"] += score

        # 2. è®¡ç®—å‘é‡æ£€ç´¢çš„ RRF åˆ†æ•°
        for rank, result in enumerate(vector_results, start=1):
            knowledge_no = result["knowledge_no"]
            score = 1.0 / (k + rank)
            if knowledge_no not in rrf_scores:
                rrf_scores[knowledge_no] = {
                    "knowledge_no": knowledge_no,
                    "doc_id": result["doc_id"],
                    "title": result["title"],
                    "content": result["content"],
                    "chunk_index": result["chunk_index"],
                    "rrf_score": 0.0,
                    "keyword_rank": None,
                    "vector_rank": rank,
                }
            else:
                rrf_scores[knowledge_no]["vector_rank"] = rank
            rrf_scores[knowledge_no]["rrf_score"] += score

        # 3. æŒ‰ RRF åˆ†æ•°æ’åº
        sorted_results = sorted(
            rrf_scores.values(), key=lambda x: x["rrf_score"], reverse=True
        )

        logger.info(f"âœ… RRF èåˆå®Œæˆ: {len(sorted_results)} ä¸ªå”¯ä¸€çŸ¥è¯†ç‚¹")
        return sorted_results

    def rrf_search(
        self, query: str, top_k: int = 5, rrf_k: int = 60
    ) -> List[Dict[str, Any]]:
        """
        åŸºäº RRF çš„æ··åˆæ£€ç´¢ï¼ˆç”¨äº A/B æµ‹è¯•ï¼‰

        æµç¨‹ï¼š
        1. åˆ†åˆ«æ‰§è¡Œ BM25 å…³é”®è¯æ£€ç´¢å’Œå‘é‡æ£€ç´¢
        2. ä½¿ç”¨ RRF ç®—æ³•èåˆä¸¤è·¯ç»“æœ
        3. æŒ‰ knowledge_no å»é‡ï¼ˆå·²åœ¨ RRF èåˆä¸­å®Œæˆï¼‰

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›çš„çŸ¥è¯†ç‚¹æ•°é‡
            rrf_k: RRF å¹³æ»‘å‚æ•°

        Returns:
            List[Dict]: æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # 1. é¢„å¤„ç†æŸ¥è¯¢
            query_segmented = self.text_processor.segment_chinese(query)
            query_vector = self.embedding_service.embed_query(query)

            logger.info(f"ğŸ” å¼€å§‹ RRF æ··åˆæ£€ç´¢: {query}")

            # 2. åˆ†åˆ«æ‰§è¡Œä¸¤è·¯æ£€ç´¢ï¼ˆå¬å›æ›´å¤šå€™é€‰ï¼‰
            keyword_results = self._keyword_search(query_segmented, top_k=top_k * 3)
            vector_results = self._vector_search(query_vector, top_k=top_k * 3)

            logger.info(f"   å…³é”®è¯æ£€ç´¢: {len(keyword_results)} ä¸ªç»“æœ")
            logger.info(f"   å‘é‡æ£€ç´¢: {len(vector_results)} ä¸ªç»“æœ")

            # 3. RRF èåˆæ’åº
            fused_results = self._rrf_fusion(keyword_results, vector_results, k=rrf_k)

            # 4. å– Top-K
            final_results = fused_results[:top_k]

            logger.info(f"âœ… RRF æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªçŸ¥è¯†ç‚¹")
            return final_results

        except Exception as e:
            logger.error(f"âŒ RRF æ£€ç´¢å¤±è´¥: {e}")
            raise

    def retrieve(
        self, query: str, top_k: int = 5, return_full_content: bool = True
    ) -> List[Dict[str, Any]]:
        """
        å®Œæ•´çš„æ£€ç´¢æµç¨‹ï¼ˆRAG V2.0ï¼‰

        æµç¨‹ï¼š
        1. æ··åˆæ£€ç´¢ï¼ˆBM25 + å‘é‡ï¼‰
        2. Collapse æŠ˜å å»é‡
        3. è·å–çˆ¶æ–‡æ¡£å®Œæ•´å†…å®¹

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›çš„çŸ¥è¯†ç‚¹æ•°é‡
            return_full_content: æ˜¯å¦è¿”å›å®Œæ•´å†…å®¹ï¼ˆParentï¼‰

        Returns:
            List[Dict]: æ£€ç´¢ç»“æœï¼ŒåŒ…å«å®Œæ•´å†…å®¹
        """
        try:
            # 1. æ··åˆæ£€ç´¢
            search_results = self.hybrid_search(query, top_k=top_k)

            if not search_results:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡æ¡£")
                return []

            # 2. æå– knowledge_no
            knowledge_nos = [r["knowledge_no"] for r in search_results]

            # 3. è·å–çˆ¶æ–‡æ¡£
            if return_full_content:
                parent_docs = self.get_parent_documents(knowledge_nos)

                # 4. åˆå¹¶ç»“æœ
                for result in search_results:
                    kno = result["knowledge_no"]
                    result["full_content"] = parent_docs.get(kno, "")

            return search_results

        except Exception as e:
            logger.error(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
            raise


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import sys
    import os
    from infrastructure.logger import logger

    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    sys.path.insert(0, project_root)

    print("\n" + "=" * 60)
    print("æµ‹è¯• ES æ··åˆæ£€ç´¢æœåŠ¡ (RAG V2.0)")
    print("=" * 60)

    # åˆå§‹åŒ–æœåŠ¡
    service = ESRetrievalService()

    # æµ‹è¯•æŸ¥è¯¢
    test_queries = ["è”æƒ³æ‰‹æœºK900å¦‚ä½•æ’æ‹”SIMå¡", "ç”µæ± ç»­èˆªé—®é¢˜", "å¦‚ä½•è¿æ¥WiFi"]

    for query in test_queries:
        print(f"\n{'='*60}")
        print(f"æŸ¥è¯¢: {query}")
        print(f"{'='*60}")

        try:
            results = service.retrieve(query, top_k=3)

            if results:
                print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªç»“æœ:\n")
                for i, result in enumerate(results, 1):
                    print(f"--- ç»“æœ {i} ---")
                    print(f"Knowledge No: {result['knowledge_no']}")
                    print(f"æ ‡é¢˜: {result['title'][:50]}...")
                    print(f"åˆ†æ•°: {result['score']:.4f}")
                    print(f"å®Œæ•´å†…å®¹é•¿åº¦: {len(result.get('full_content', ''))} å­—ç¬¦")
                    print()
            else:
                print("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…ç»“æœ")

        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
