"""
ES å…¥åº“å¤„ç†å™¨
è´Ÿè´£å°†æ–‡æ¡£å¤„ç†å¹¶å†™å…¥ Elasticsearchï¼ˆN+1 å­˜å‚¨æ¨¡å¼ï¼‰
"""

from typing import List, Dict, Any
import os
from datetime import datetime
import uuid

try:
    from infrastructure.es_client import ESClient
    from services.embedding_service import EmbeddingService
    from services.text_processor import TextProcessor
    from config.settings import settings
    from infrastructure.logger import logger
except ModuleNotFoundError:
    import sys

    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    sys.path.insert(0, project_root)
    from infrastructure.es_client import ESClient
    from services.embedding_service import EmbeddingService
    from services.text_processor import TextProcessor
    from config.settings import settings
    from infrastructure.logger import logger


class ESIngestionProcessor:
    """ES å…¥åº“å¤„ç†å™¨ - å®ç° N+1 å­˜å‚¨æ¨¡å¼"""

    def __init__(self):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        self.es_client = ESClient()
        self.embedding_service = EmbeddingService()
        self.text_processor = TextProcessor()
        self.index_name = settings.ES_INDEX_NAME
        logger.info(f"âœ… ES å…¥åº“å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸï¼Œç´¢å¼•: {self.index_name}")

    def ingest_file(
        self, file_path: str, title: str = "", knowledge_no: str = None
    ) -> int:
        """
        å¤„ç†å¹¶å…¥åº“å•ä¸ªæ–‡ä»¶ï¼ˆN+1 æ¨¡å¼ï¼‰

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            title: æ–‡æ¡£æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
            knowledge_no: çŸ¥è¯†ç‚¹ç¼–å·ï¼ˆå¯é€‰ï¼‰ã€‚å¦‚æœä¸ä¼ ï¼Œåˆ™æ ¹æ® file_path ç”Ÿæˆã€‚

        Returns:
            int: å…¥åº“çš„æ–‡æ¡£æ•°é‡
        """
        try:
            # 1. è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, "r", encoding="utf-8") as f:
                full_content = f.read()

            # å¦‚æœæ²¡æœ‰æä¾›æ ‡é¢˜ï¼Œä½¿ç”¨æ–‡ä»¶å
            if not title:
                title = os.path.splitext(os.path.basename(file_path))[0]

            logger.info(f"ğŸ“„ å¼€å§‹å¤„ç†æ–‡ä»¶: {title}")

            # 2. ç”Ÿæˆ knowledge_no
            if not knowledge_no:
                knowledge_no = TextProcessor.generate_knowledge_no(file_path)

            # 3. è°ƒç”¨æ ¸å¿ƒå…¥åº“é€»è¾‘
            return self.ingest_content(
                content=full_content,
                title=title,
                knowledge_no=knowledge_no,
                source_path=file_path
            )

        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶å…¥åº“å¤±è´¥: {e}")
            raise

    def ingest_content(
        self, content: str, title: str, asset_uuid: str, knowledge_no: str, source_path: str = ""
    ) -> int:
        """
        å¤„ç†å¹¶å…¥åº“å†…å­˜ä¸­çš„æ–‡æœ¬å†…å®¹

        Args:
            content: æ–‡æœ¬å†…å®¹
            title: æ ‡é¢˜
            asset_uuid: å†…éƒ¨ä¸šåŠ¡ID (ç”¨äºç”Ÿæˆ ES _id)
            knowledge_no: å¤–éƒ¨çŸ¥è¯†ç¼–å· (ä»…å­˜å‚¨ç”¨äºå±•ç¤º)
            source_path: æºè·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºè®°å½•å…ƒæ•°æ®ï¼‰

        Returns:
            int: å…¥åº“æ•°é‡
        """
        try:
            logger.info(f"ğŸ”‘ Processing Asset UUID: {asset_uuid} (External: {knowledge_no})")

            # --- MD5 ä¼˜åŒ–å¼€å§‹ ---
            import hashlib
            content_md5 = hashlib.md5(content.encode("utf-8")).hexdigest()

            try:
                # å°è¯•è·å– Parent æ–‡æ¡£ (ID ä½¿ç”¨ asset_uuid)
                existing_parent = self.es_client.get_document(
                    self.index_name, f"{asset_uuid}_parent"
                )
                if existing_parent:
                    old_md5 = existing_parent.get("content_md5")
                    if old_md5 == content_md5:
                        logger.info(f"â­ï¸ æ–‡æ¡£å†…å®¹æœªå˜æ›´ (MD5: {content_md5})ï¼Œè·³è¿‡å…¥åº“")
                        return 0
                    else:
                        logger.info(
                            f"ğŸ”„ æ–‡æ¡£å†…å®¹å˜æ›´ (Old: {old_md5} -> New: {content_md5})ï¼Œå‡†å¤‡æ›´æ–°"
                        )
            except Exception as e:
                logger.warning(f"âš ï¸ æ£€æŸ¥æ—§ç‰ˆæœ¬å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œå…¥åº“: {e}")
            # --- MD5 ä¼˜åŒ–ç»“æŸ ---

            # æ¸…ç†æ—§æ•°æ® (ä½¿ç”¨ asset_uuid æŸ¥è¯¢)
            deleted_count = self.es_client.delete_by_query(
                self.index_name, query={"term": {"asset_uuid": asset_uuid}}
            )
            if deleted_count > 0:
                logger.info(f"ğŸ§¹ æ¸…ç†æ—§æ•°æ®: åˆ é™¤äº† {deleted_count} æ¡å…³è”æ–‡æ¡£")

            # æ–‡æœ¬åˆ‡åˆ†
            chunks = self.text_processor.split_text(content, title)
            logger.info(f"âœ‚ï¸ æ–‡æœ¬åˆ‡åˆ†å®Œæˆ: {len(chunks)} ä¸ª chunks")

            # å‡†å¤‡æ–‡æ¡£
            documents = self._prepare_documents(
                asset_uuid=asset_uuid,
                knowledge_no=knowledge_no,
                title=title,
                full_content=content,
                chunks=chunks,
                file_path=source_path,
                content_md5=content_md5,
            )

            # æ‰¹é‡å†™å…¥
            success_count = self._bulk_write_to_es(documents)

            logger.info(f"âœ… å…¥åº“å®Œæˆ: {title}, å…± {success_count} æ¡è®°å½•")
            return success_count

        except Exception as e:
            logger.error(f"âŒ å†…å®¹å…¥åº“å¤±è´¥: {e}")
            raise

    def _prepare_documents(
        self,
        asset_uuid: str,
        knowledge_no: str,
        title: str,
        full_content: str,
        chunks: List[Dict[str, Any]],
        file_path: str,
        content_md5: str = "",
    ) -> List[Dict[str, Any]]:
        """
        å‡†å¤‡å†™å…¥ ES çš„æ–‡æ¡£ï¼ˆN+1 æ¨¡å¼ï¼‰
        """
        documents = []
        current_time = datetime.utcnow().isoformat()

        # 1. åˆ›å»º Parent æ–‡æ¡£
        parent_doc = {
            "doc_id": f"{asset_uuid}_parent",
            "asset_uuid": asset_uuid,
            "knowledge_no": knowledge_no,
            "doc_type": "parent",
            "title": title,
            "full_content": full_content,
            "file_path": file_path,
            "content_md5": content_md5,
            "created_at": current_time,
        }
        documents.append(parent_doc)
        logger.info(f"ğŸ“ åˆ›å»º Parent æ–‡æ¡£: {parent_doc['doc_id']}")

        # 2. å¤„ç† Chunks
        chunk_texts = [chunk["content"] for chunk in chunks]
        logger.info(f"ğŸ”„ å¼€å§‹å‘é‡åŒ– {len(chunk_texts)} ä¸ª chunks...")
        chunk_vectors = self.embedding_service.embed_batch(chunk_texts)
        title_segmented = self.text_processor.segment_chinese(title)

        for i, (chunk, vector) in enumerate(zip(chunks, chunk_vectors)):
            content_segmented = self.text_processor.segment_chinese(chunk["content"])

            chunk_doc = {
                "doc_id": f"{asset_uuid}_chunk_{i}",
                "asset_uuid": asset_uuid,
                "knowledge_no": knowledge_no,
                "doc_type": "chunk",
                "title": title_segmented,
                "content": content_segmented,
                "content_vector": vector,
                "chunk_index": chunk["chunk_index"],
                "file_path": file_path,
                "created_at": current_time,
            }
            documents.append(chunk_doc)

        return documents

    def _bulk_write_to_es(self, documents: List[Dict[str, Any]]) -> int:
        """
        æ‰¹é‡å†™å…¥æ–‡æ¡£åˆ° ES

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨

        Returns:
            int: æˆåŠŸå†™å…¥çš„æ–‡æ¡£æ•°é‡
        """
        try:
            # æ„é€  bulk æ“ä½œ
            actions = []
            for doc in documents:
                action = {
                    "_index": self.index_name,
                    "_id": doc["doc_id"],
                    "_source": doc,
                }
                actions.append(action)

            # æ‰§è¡Œæ‰¹é‡å†™å…¥
            success, failed = self.es_client.bulk_index(actions)

            # failed æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«å¤±è´¥çš„æ“ä½œ
            failed_count = len(failed) if isinstance(failed, list) else 0
            if failed_count > 0:
                logger.warning(
                    f"âš ï¸ éƒ¨åˆ†æ–‡æ¡£å†™å…¥å¤±è´¥: æˆåŠŸ {success}, å¤±è´¥ {failed_count}"
                )
                # è®°å½•å¤±è´¥çš„æ–‡æ¡£ ID
                for fail_item in failed[:5]:  # åªè®°å½•å‰5ä¸ªå¤±è´¥é¡¹
                    logger.error(f"  å¤±è´¥é¡¹: {fail_item}")

            return success

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡å†™å…¥ ES å¤±è´¥: {e}")
            raise


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import sys
    from infrastructure.logger import logger

    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    sys.path.insert(0, project_root)

    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    test_file = "/tmp/test_document.md"
    with open(test_file, "w", encoding="utf-8") as f:
        f.write("# æµ‹è¯•æ–‡æ¡£\n\nè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œç”¨äºéªŒè¯ ES å…¥åº“åŠŸèƒ½ã€‚\n\n" * 100)

        # æµ‹è¯•å…¥åº“
    processor = ESIngestionProcessor()
    count = processor.ingest_file(test_file, "æµ‹è¯•æ–‡æ¡£æ ‡é¢˜")
    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼Œå…¥åº“ {count} æ¡è®°å½•")
