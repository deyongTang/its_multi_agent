import os.path
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import aiofiles
from fastapi import APIRouter, UploadFile, File, HTTPException
from fastapi.concurrency import run_in_threadpool

from services.ingestion.ingestion_processor import IngestionProcessor
from schemas.schema import UploadResponse, QueryRequest, QueryResponse
from services.retrieval_service import RetrievalService
from business_logic.query_service import QueryService

from infrastructure.es_client import ESClient
from services.es_ingestion_processor import ESIngestionProcessor
from services.es_retrieval_service import ESRetrievalService

# å¼•å…¥ä¸šåŠ¡æœåŠ¡
from business_logic.document_sync_service import DocumentSyncService
from business_logic.crawler_service import CrawlerService
from business_logic.ingestion_worker_service import IngestionWorkerService
from datetime import datetime

# ä½¿ç”¨æ–°çš„æ—¥å¿—ç³»ç»Ÿ
from infrastructure.logger import logger

from fastapi import BackgroundTasks

# 1.åˆ›å»ºAPIRouter
router = APIRouter()

# ... (existing code)

# --- ä»»åŠ¡è°ƒåº¦æ¥å£ (Task Endpoints) ---

def _run_crawl_task(start_id: int, end_id: int):
    """
    [åå°ä»»åŠ¡] æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
    Pipeline 1: Crawler -> OSS
    æ­¥éª¤: æŠ“å– -> æ¸…æ´— -> å­˜å…¥ MinIO -> æ•°æ®åº“æ ‡è®°ä¸º NEW
    """
    logger.info(f"ğŸš€ [åå°ä»»åŠ¡å¯åŠ¨] çˆ¬è™«ä»»åŠ¡: ID {start_id} åˆ° {end_id}")
    crawler = CrawlerService()
    crawler.crawl_range(start_id, end_id)
    logger.info("âœ… [åå°ä»»åŠ¡å®Œæˆ] çˆ¬è™«ä»»åŠ¡ç»“æŸ")

@router.post("/tasks/crawl", summary="è§¦å‘åå°çˆ¬è™«ä»»åŠ¡ (Pipeline 1)")
async def trigger_crawl_task(
    background_tasks: BackgroundTasks,
    start_id: int = 1,
    end_id: int = 100
):
    """
    å¯åŠ¨åå°çˆ¬è™«ä»»åŠ¡ (Pipeline 1: Crawler -> OSS)
    
    - **start_id**: èµ·å§‹çŸ¥è¯†ç‚¹ ID
    - **end_id**: ç»“æŸçŸ¥è¯†ç‚¹ ID
    
    æ­¤æ¥å£ä¼šç«‹å³è¿”å›ï¼Œå¹¶åœ¨åå°å¼‚æ­¥æ‰§è¡ŒæŠ“å–é€»è¾‘ã€‚
    """
    background_tasks.add_task(_run_crawl_task, start_id, end_id)
    return {
        "status": "accepted",
        "message": f"çˆ¬è™«ä»»åŠ¡å·²æäº¤ (Range: {start_id}-{end_id})",
        "task_type": "crawler"
    }

def _run_ingestion_task(batch_size: int, retry: bool):
    """
    [åå°ä»»åŠ¡] æ‰§è¡Œå…¥åº“ä»»åŠ¡
    Pipeline 2: MinIO -> ES
    æ­¥éª¤: æ‰«æ NEW æ–‡æ¡£ -> ä¸‹è½½ -> åˆ‡åˆ†/å‘é‡åŒ– -> å†™å…¥ ES -> æ ‡è®°ä¸º INGESTED
    """
    logger.info(f"ğŸš€ [åå°ä»»åŠ¡å¯åŠ¨] å…¥åº“ä»»åŠ¡ (Batch: {batch_size}, Retry: {retry})")
    worker = IngestionWorkerService()
    
    # 1. å¤„ç†çŠ¶æ€ä¸º NEW çš„æ–°æ–‡æ¡£
    worker.process_pending_documents(batch_size=batch_size)
    
    # 2. (å¯é€‰) é‡è¯•çŠ¶æ€ä¸º FAILED çš„æ–‡æ¡£
    if retry:
        worker.retry_failed_documents()
        
    logger.info("âœ… [åå°ä»»åŠ¡å®Œæˆ] å…¥åº“ä»»åŠ¡ç»“æŸ")

@router.post("/tasks/ingest", summary="è§¦å‘åå°å…¥åº“ä»»åŠ¡ (Pipeline 2)")
async def trigger_ingest_task(
    background_tasks: BackgroundTasks,
    batch_size: int = 100,
    retry: bool = False
):
    """
    å¯åŠ¨åå°å…¥åº“ä»»åŠ¡ (Pipeline 2: MinIO -> ES)
    
    - **batch_size**: æœ¬æ¬¡å¤„ç†çš„æ–‡æ¡£æ•°é‡
    - **retry**: æ˜¯å¦é‡è¯•ä¹‹å‰å¤±è´¥çš„ä»»åŠ¡
    
    æ­¤æ¥å£ä¼šç«‹å³è¿”å›ï¼Œå¹¶åœ¨åå°å¼‚æ­¥æ‰§è¡Œå…¥åº“é€»è¾‘ã€‚
    """
    background_tasks.add_task(_run_ingestion_task, batch_size, retry)
    return {
        "status": "accepted",
        "message": "å…¥åº“ä»»åŠ¡å·²æäº¤",
        "task_type": "ingestion"
    }

@router.post("/test/upload_to_oss", summary="æµ‹è¯•ï¼šä¸Šä¼ æ–‡ä»¶åˆ° MinIO å¹¶æ³¨å†Œèµ„äº§")
async def test_upload_to_oss(
    file: UploadFile = File(...),
    knowledge_no: str = "test-001"
):
    """
    æµ‹è¯•æ¥å£ï¼šä¸Šä¼ æ–‡ä»¶åˆ° MinIO å¹¶æ³¨å†Œåˆ°æ•°æ®åº“ (çŠ¶æ€: NEW)
    ç”¨äºéªŒè¯ 'Crawler -> OSS' é“¾è·¯
    """
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        
        # åˆå§‹åŒ–æœåŠ¡
        sync_service = DocumentSyncService()
        
        # æ‰§è¡Œä¸Šä¼ 
        result = await run_in_threadpool(
            sync_service.upload_document,
            file_content=content,
            knowledge_no=knowledge_no,
            source_update_time=datetime.now()
        )
        
        return {
            "status": "success",
            "message": "æ–‡ä»¶å·²ä¸Šä¼ åˆ° MinIO",
            "data": result
        }
        
    except Exception as e:
        logger.error(f"æµ‹è¯•ä¸Šä¼ å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# 2. å»¶è¿Ÿåˆå§‹åŒ–æœåŠ¡å®ä¾‹ï¼ˆé¿å…åœ¨å¯¼å…¥æ—¶åˆ›å»ºï¼Œæ­¤æ—¶ç¯å¢ƒå˜é‡å¯èƒ½æœªè®¾ç½®ï¼‰
_ingestion_processor = None
_retrieval_service = None
_query_service = None
_es_ingestion_processor = None
_es_retrieval_service = None


def get_ingestion_processor():
    global _ingestion_processor
    if _ingestion_processor is None:
        _ingestion_processor = IngestionProcessor()
    return _ingestion_processor


def get_retrieval_service():
    global _retrieval_service
    if _retrieval_service is None:
        _retrieval_service = RetrievalService()
    return _retrieval_service


def get_query_service():
    global _query_service
    if _query_service is None:
        _query_service = QueryService()
    return _query_service


def get_es_ingestion_processor():
    global _es_ingestion_processor
    if _es_ingestion_processor is None:
        _es_ingestion_processor = ESIngestionProcessor()
    return _es_ingestion_processor


def get_es_retrieval_service():
    global _es_retrieval_service
    if _es_retrieval_service is None:
        _es_retrieval_service = ESRetrievalService()
    return _es_retrieval_service


# IO(å¯¹æ–‡ä»¶è¯»å†™) æ‰§è¡ŒSQL ç½‘ç»œè¯·æ±‚ å…¸å‹è€—æ—¶ä»»åŠ¡
@router.post("/upload", response_model=UploadResponse, summary="å¤„ç†çŸ¥è¯†åº“ä¸Šä¼ ")
async def upload_file(file: UploadFile = File(...)):
    # "0430-è”æƒ³æ‰‹æœºK900å¸¸è§é—®é¢˜æ±‡æ€».md"
    temp_file_path = ""

    try:
        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦å­˜åœ¨
        if not file.filename:
            raise HTTPException(status_code=400, detail="æ–‡ä»¶åä¸èƒ½ä¸ºç©º")

        file_suffix = os.path.splitext(file.filename)[1]
        # 1. å¤„ç†ä¸´æ—¶æ–‡ä»¶
        async with aiofiles.tempfile.NamedTemporaryFile(
            delete=False, suffix=file_suffix
        ) as temp_file:

            # a. è¯»å–ä¸Šä¼ æ–‡ä»¶çš„å†…å®¹ # å¯¹è±¡ï¼ˆå¼‚æ­¥åç¨‹ï¼‰ç¼“å†²åŒºã€1Mã€‘ç©ºé—´
            while content := await file.read(1024 * 1024):
                # b. å°†è¯»å–åˆ°ä¸Šä¼ æ–‡ä»¶çš„å†…å®¹å†™å…¥åˆ°ä¸´æ—¶æ–‡ä»¶
                await temp_file.write(content)

            # c. è·å–ä¸´æ—¶æ–‡ä»¶çš„è·¯å¾„ # C:\Users\Administrator\AppData\Local\Temp\tmpe1puxhk7
            temp_file_path = str(temp_file.name)

        # 2. ç£ç›˜å†™å…¥å®Œæˆ,å…¥åº“æ“ä½œ  # TODO(å»é‡)
        chunks_added = await run_in_threadpool(
            get_ingestion_processor().ingest_file, temp_file_path
        )
        print(f"ä¸´æ—¶æ–‡ä»¶è·¯å¾„:{temp_file_path}")

        # 3.æ„å»ºæ–‡ä»¶ä¸Šä¼ çš„å“åº”å¯¹è±¡
        return UploadResponse(
            status="success",
            message="æ–‡æ¡£ä¸Šä¼ çŸ¥è¯†åº“æˆåŠŸ",
            file_name=file.filename,
            chunks_added=chunks_added,
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ åˆ°çŸ¥è¯†åº“å¤±è´¥:{str(e)}")

    finally:
        # 4. æ¸…ç©ºä¸´æ—¶æ–‡ä»¶è·¯å¾„(ç£ç›˜ç©ºé—´ä¸è¶³)
        if temp_file_path and os.path.exists(temp_file_path):
            os.remove(temp_file_path)
            logger.info(f"ä¸´æ—¶æ–‡ä»¶:{temp_file_path}å·²åˆ é™¤...")


@router.post("/upload_es", response_model=UploadResponse, summary="ä¸Šä¼ æ–‡æ¡£åˆ° Elasticsearch")
async def upload_file_to_es(file: UploadFile = File(...)):
    """
    ä¸Šä¼ æ–‡æ¡£åˆ° Elasticsearchï¼ˆN+1 å­˜å‚¨æ¨¡å¼ï¼‰
    - N ä¸ªåˆ‡ç‰‡ç”¨äºæœç´¢ï¼ˆå¸¦å‘é‡ï¼‰
    - 1 ä¸ªå…¨æ–‡ç”¨äºå±•ç¤º
    """
    temp_file_path = ""

    try:
        # 1. æ£€æŸ¥æ–‡ä»¶å
        if not file.filename:
            raise HTTPException(status_code=400, detail="æ–‡ä»¶åä¸èƒ½ä¸ºç©º")

        file_suffix = os.path.splitext(file.filename)[1]

        # 2. ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        async with aiofiles.tempfile.NamedTemporaryFile(
            delete=False, suffix=file_suffix
        ) as temp_file:
            while content := await file.read(1024 * 1024):
                await temp_file.write(content)
            temp_file_path = str(temp_file.name)

        logger.info(f"ğŸ“ ä¸´æ—¶æ–‡ä»¶å·²ä¿å­˜: {temp_file_path}")

        # 3. ç”Ÿæˆç¨³å®šçš„ knowledge_no (åŸºäºåŸå§‹æ–‡ä»¶å)
        # å³ä½¿æ¯æ¬¡ä¸Šä¼ ç”Ÿæˆä¸åŒçš„ä¸´æ—¶æ–‡ä»¶è·¯å¾„ï¼Œåªè¦åŸå§‹æ–‡ä»¶åç›¸åŒï¼ŒID å°±ç›¸åŒ
        import hashlib
        knowledge_no = hashlib.md5(file.filename.encode('utf-8')).hexdigest()
        logger.info(f"ğŸ”‘ ç”Ÿæˆ stable ID: {knowledge_no} (from {file.filename})")

        # 4. è°ƒç”¨ ES å…¥åº“å¤„ç†å™¨
        chunks_added = await run_in_threadpool(
            get_es_ingestion_processor().ingest_file,
            temp_file_path,
            os.path.splitext(file.filename)[0],  # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºæ ‡é¢˜
            knowledge_no  # ä¼ å…¥ç¨³å®šçš„ ID
        )

        # 5. è¿”å›å“åº”
        return UploadResponse(
            status="success",
            message="æ–‡æ¡£å·²æˆåŠŸä¸Šä¼ åˆ° Elasticsearch",
            file_name=file.filename,
            chunks_added=chunks_added,
        )

    except Exception as e:
        logger.error(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")

    finally:
        # 6. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_file_path and os.path.exists(temp_file_path):
            os.remove(temp_file_path)
            logger.info(f"ğŸ—‘ï¸ ä¸´æ—¶æ–‡ä»¶å·²åˆ é™¤: {temp_file_path}")


@router.post("/query", summary="æŸ¥è¯¢çŸ¥è¯†åº“ï¼ˆæµå¼è¾“å‡ºï¼‰")
async def query_knowledge(request: QueryRequest):
    """
    æµå¼æŸ¥è¯¢çŸ¥è¯†åº“æ¥å£
    è¿”å› Server-Sent Events (SSE) æ ¼å¼çš„æµå¼å“åº”
    """
    if not request.question.strip():
        raise HTTPException(status_code=400, detail="é—®é¢˜ä¸èƒ½ä¸ºç©º")

    async def generate_stream():
        """ç”Ÿæˆæµå¼å“åº”çš„å¼‚æ­¥ç”Ÿæˆå™¨"""
        try:
            logger.info(f"ğŸ” åŸå§‹é—®é¢˜: {request.question}")

            # 1. æŸ¥è¯¢é‡å†™ (åˆ©ç”¨ LLM ä¼˜åŒ–æ£€ç´¢è¯)
            query_service = get_query_service()
            rewritten_query = await run_in_threadpool(query_service.rewrite_query, request.question)

            logger.info(f"âœ¨ é‡å†™åé—®é¢˜: {rewritten_query}")

            # 2. ä½¿ç”¨ ES æ··åˆæ£€ç´¢ï¼ˆæ–°æ–¹æ¡ˆ - RAG V2.0ï¼‰
            es_results = await run_in_threadpool(
                get_es_retrieval_service().retrieve,
                rewritten_query,
                top_k=5,  # é™åˆ¶ä¸º 5 ä¸ªæ–‡æ¡£
                return_full_content=True
            )

            # å°† ES ç»“æœè½¬æ¢ä¸º LangChain Document æ ¼å¼
            from langchain_core.documents import Document
            docs = [
                Document(
                    page_content=result.get("full_content", result.get("content", "")),
                    metadata={
                        "knowledge_no": result.get("knowledge_no"),
                        "title": result.get("title"),
                        "score": result.get("score", 0)
                    }
                )
                for result in es_results
            ]

            logger.info(f"ğŸ“š ES æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£")
            logger.info(f"ğŸ¤– å¼€å§‹ç”Ÿæˆç­”æ¡ˆ | ä¸Šä¸‹æ–‡æ–‡æ¡£æ•°: {len(docs)}")

            # 3. æµå¼ç”Ÿæˆç­”æ¡ˆ
            for chunk in query_service.generate_answer_stream(request.question, docs):
                # ä»¥ SSE æ ¼å¼è¾“å‡º
                yield f"data: {chunk}\n\n"

            logger.info("âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢å‡ºé”™: {e}")
            yield f"data: [ERROR] {str(e)}\n\n"

    from fastapi.responses import StreamingResponse
    return StreamingResponse(generate_stream(), media_type="text/event-stream")

@router.get("/es", summary="æµ‹è¯• ES è¿æ¥")
def testEs():
    """æµ‹è¯• Elasticsearch è¿æ¥å’ŒæŸ¥è¯¢åŠŸèƒ½"""
    try:
        es_client = ESClient()
        index_name = "test_index"

        # ES 8.x æ­£ç¡®çš„æŸ¥è¯¢æ ¼å¼ï¼šå®Œæ•´çš„æŸ¥è¯¢ DSL
        query = {
            "query": {
                "match_all": {}
            }
        }

        results = es_client.search(index_name=index_name, query=query)
        print("æœç´¢ç»“æœ:", results)

        # è¿”å›æµ‹è¯•ç»“æœ
        return {
            "status": "success",
            "message": "ES è¿æ¥æµ‹è¯•æˆåŠŸ",
            "results": results
        }
    except Exception as e:
        print(f"ES æµ‹è¯•å¤±è´¥: {e}")
        return {
            "status": "error",
            "message": f"ES è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}"
        }